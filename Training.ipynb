{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AbstractiveSummarization.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_37T4ZCJZYE",
        "colab_type": "code",
        "outputId": "51a8d14c-8cbb-4899-bb14-d26e6aca5f6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0710 06:31:00.955379 140553886504832 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTyKEcbuJb8W",
        "colab_type": "code",
        "outputId": "94ed3fda-4820-443b-eefd-bf58ed621aa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import collections\n",
        "import pickle\n",
        "import numpy as np\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from gensim.test.utils import get_tmpfile\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPhs8sgQLyRM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#default path for the folder inside google drive\n",
        "default_path = \"drive/My Drive/Colab Notebooks/\"\n",
        "\n",
        "#path for getting training data\n",
        "train_article_path = default_path + \"sumdata/train/train.article.txt\" \n",
        "\n",
        "#path for training text output (headline)\n",
        "train_title_path   = default_path + \"sumdata/train/train.title.txt\"\n",
        "\n",
        "#path for validation text (article)\n",
        "valid_article_path = default_path + \"sumdata/train/valid.article.filter.txt\"\n",
        "\n",
        "#path for validation text output(headline)\n",
        "valid_title_path   = default_path + \"sumdata/train/valid.title.filter.txt\"\n",
        "input_size_ = 200000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_-0-1OLME7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_str(sentence):\n",
        "    sentence = re.sub(\"[#]+\", \"#\", sentence)\n",
        "    return sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EkSpxdqMHUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_text_list(data_path, toy):\n",
        "    with open (data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        if not toy:\n",
        "            return [clean_str(x.strip()) for x in f.readlines()][:input_size_]\n",
        "        else:\n",
        "            return [clean_str(x.strip()) for x in f.readlines()][:1000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Qj0hQpjMNGg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dict(step, toy=False):\n",
        "    if step == \"train\":\n",
        "        train_article_list = get_text_list(train_article_path, toy)\n",
        "        train_title_list = get_text_list(train_title_path, toy)\n",
        "\n",
        "        '''words = list()\n",
        "        for sentence in train_article_list + train_title_list:\n",
        "            for word in word_tokenize(sentence):\n",
        "                words.append(word)\n",
        "\n",
        "        word_counter = collections.Counter(words).most_common()\n",
        "        #print(word_counter)\n",
        "        # ('#', 363119), ('the', 307009), (',', 204614), ('to', 202066) ouuput \n",
        "        word_dict = dict()\n",
        "        word_dict[\"<padding>\"] = 0\n",
        "        word_dict[\"<unk>\"] = 1\n",
        "        word_dict[\"<s>\"] = 2\n",
        "        word_dict[\"</s>\"] = 3\n",
        "        # we are creating the word to int dictionary \n",
        "        for word, _ in word_counter:\n",
        "            word_dict[word] = len(word_dict) \n",
        "\n",
        "        with open(default_path + \"word_dict.pickle\", \"wb\") as f:\n",
        "            pickle.dump(word_dict, f)'''\n",
        "        with open(default_path + \"word_dict.pickle\", \"rb\") as f:\n",
        "            word_dict = pickle.load(f)\n",
        "\n",
        "    elif step == \"valid\":\n",
        "        with open(default_path + \"word_dict.pickle\", \"rb\") as f:\n",
        "            word_dict = pickle.load(f)\n",
        "     \n",
        "\n",
        "    reversed_dict = dict(zip(word_dict.values(), word_dict.keys()))\n",
        "\n",
        "    '''article_max_len = max([len(word_tokenize(d)) for d in train_article_list])\n",
        "    print(article_max_len)\n",
        "    summary_max_len = max([len(word_tokenize(d)) for d in train_title_list])\n",
        "    print(summary_max_len)'''\n",
        "    article_max_len = 150\n",
        "    summary_max_len = 50\n",
        "\n",
        "    return word_dict, reversed_dict, article_max_len, summary_max_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj0t81LLNHiO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dataset(step,article_list, title_list, word_dict, article_max_len, summary_max_len, toy=False):\n",
        "    if step == \"train\":\n",
        "        article_list = [clean_str(x.strip()) for x in article_list]\n",
        "        title_list   = [clean_str(x.strip()) for x in title_list]\n",
        "    elif step == \"valid\":\n",
        "        article_list = [clean_str(x.strip()) for x in article_list]\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    #print(\"Type of batch \",type(article_list))\n",
        "    #print(\"TYpe of batch \",type(title_list))\n",
        "    #print(article_list)\n",
        "    #print(\"Next\")\n",
        "    #print(title_list)\n",
        "    x = [word_tokenize(d) for d in article_list]\n",
        "    x = [[word_dict.get(w, word_dict[\"<unk>\"]) for w in d] for d in x]\n",
        "    #print(\"Hi Prabha, here you are\")\n",
        "    #print(x)\n",
        "    x = [d[:article_max_len] for d in x]\n",
        "    x = [d + (article_max_len - len(d)) * [word_dict[\"<padding>\"]] for d in x]\n",
        "    \n",
        "    if step == \"valid\":\n",
        "        return x\n",
        "    else:        \n",
        "        y = [word_tokenize(d) for d in title_list]\n",
        "        y = [[word_dict.get(w, word_dict[\"<unk>\"]) for w in d] for d in y]\n",
        "        y = [d[:(summary_max_len - 1)] for d in y]\n",
        "        return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lV7I_6x3Xeb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_init_embedding(reversed_dict, embedding_size):\n",
        "    #glove_file = default_path + \"glove/glove.6B.300d.txt\"\n",
        "    #word2vec_file = get_tmpfile(default_path + \"word2vec_format.vec\")\n",
        "    #glove2word2vec(glove_file, word2vec_file)\n",
        "    print(\"Loading Glove vectors...\")\n",
        "    #word_vectors = KeyedVectors.load_word2vec_format(word2vec_file)\n",
        "\n",
        "    with open(default_path + \"glove/model_glove_300.pkl\", 'rb') as handle:\n",
        "        word_vectors = pickle.load(handle)\n",
        "        \n",
        "    word_vec_list = list()\n",
        "    for _, word in sorted(reversed_dict.items()):\n",
        "        try:\n",
        "            word_vec = word_vectors.word_vec(word)\n",
        "        except KeyError:\n",
        "            word_vec = np.zeros([embedding_size], dtype=np.float32)\n",
        "\n",
        "        word_vec_list.append(word_vec)\n",
        "\n",
        "    # Assign random vector to <s>, </s> token\n",
        "    word_vec_list[2] = np.random.normal(0, 1, embedding_size)\n",
        "    word_vec_list[3] = np.random.normal(0, 1, embedding_size)\n",
        "\n",
        "    return np.array(word_vec_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klc0lKyIZVMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.contrib import rnn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8_eXx6hZkP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(object):\n",
        "    def __init__(self, reversed_dict, article_max_len, summary_max_len, args, forward_only=False):\n",
        "        self.vocabulary_size = len(reversed_dict)\n",
        "        self.embedding_size = args.embedding_size\n",
        "        self.num_hidden = args.num_hidden\n",
        "        self.num_layers = args.num_layers\n",
        "        self.learning_rate = args.learning_rate\n",
        "        self.beam_width = args.beam_width\n",
        "        \n",
        "        if not forward_only:\n",
        "            self.keep_prob = args.keep_prob\n",
        "        else:\n",
        "            self.keep_prob = 1.0\n",
        "        self.cell = tf.nn.rnn_cell.BasicLSTMCell\n",
        "        with tf.variable_scope(\"decoder/projection\"):\n",
        "            self.projection_layer = tf.layers.Dense(self.vocabulary_size, use_bias=False)\n",
        "\n",
        "        self.batch_size = tf.placeholder(tf.int32, (), name=\"batch_size\")\n",
        "        self.X = tf.placeholder(tf.int32, [None, article_max_len])\n",
        "        self.X_len = tf.placeholder(tf.int32, [None])\n",
        "        self.decoder_input = tf.placeholder(tf.int32, [None, summary_max_len])\n",
        "        self.decoder_len = tf.placeholder(tf.int32, [None])\n",
        "        self.decoder_target = tf.placeholder(tf.int32, [None, summary_max_len])\n",
        "        self.global_step = tf.Variable(0, trainable=False)\n",
        "        with tf.name_scope(\"embedding\"):\n",
        "            if not forward_only and args.glove:\n",
        "                init_embeddings = tf.constant(get_init_embedding(reversed_dict, self.embedding_size), dtype=tf.float32)\n",
        "            else:\n",
        "                init_embeddings = tf.random_uniform([self.vocabulary_size, self.embedding_size], -1.0, 1.0)\n",
        "            self.embeddings = tf.get_variable(\"embeddings\", initializer=init_embeddings)\n",
        "            self.encoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.X), perm=[1, 0, 2])\n",
        "            self.decoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.decoder_input), perm=[1, 0, 2])\n",
        "\n",
        "        with tf.name_scope(\"encoder\"):\n",
        "            fw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]\n",
        "            bw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]\n",
        "            fw_cells = [rnn.DropoutWrapper(cell) for cell in fw_cells]\n",
        "            bw_cells = [rnn.DropoutWrapper(cell) for cell in bw_cells]\n",
        "\n",
        "            encoder_outputs, encoder_state_fw, encoder_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
        "                fw_cells, bw_cells, self.encoder_emb_inp,\n",
        "                sequence_length=self.X_len, time_major=True, dtype=tf.float32)\n",
        "            self.encoder_output = tf.concat(encoder_outputs, 2)\n",
        "            encoder_state_c = tf.concat((encoder_state_fw[0].c, encoder_state_bw[0].c), 1)\n",
        "            encoder_state_h = tf.concat((encoder_state_fw[0].h, encoder_state_bw[0].h), 1)\n",
        "            self.encoder_state = rnn.LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)\n",
        "\n",
        "        with tf.name_scope(\"decoder\"), tf.variable_scope(\"decoder\") as decoder_scope:\n",
        "            decoder_cell = self.cell(self.num_hidden * 2)\n",
        "\n",
        "            if not forward_only:\n",
        "                attention_states = tf.transpose(self.encoder_output, [1, 0, 2])\n",
        "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
        "                    self.num_hidden * 2, attention_states, memory_sequence_length=self.X_len, normalize=True)\n",
        "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,\n",
        "                                                                   attention_layer_size=self.num_hidden * 2)\n",
        "                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size)\n",
        "                initial_state = initial_state.clone(cell_state=self.encoder_state)\n",
        "                helper = tf.contrib.seq2seq.TrainingHelper(self.decoder_emb_inp, self.decoder_len, time_major=True)\n",
        "                decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, initial_state)\n",
        "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, output_time_major=True, scope=decoder_scope)\n",
        "                self.decoder_output = outputs.rnn_output\n",
        "                self.logits = tf.transpose(\n",
        "                    self.projection_layer(self.decoder_output), perm=[1, 0, 2])\n",
        "                self.logits_reshape = tf.concat(\n",
        "                    [self.logits, tf.zeros([self.batch_size, summary_max_len - tf.shape(self.logits)[1], self.vocabulary_size])], axis=1)\n",
        "            else:\n",
        "                tiled_encoder_output = tf.contrib.seq2seq.tile_batch(\n",
        "                    tf.transpose(self.encoder_output, perm=[1, 0, 2]), multiplier=self.beam_width)\n",
        "                tiled_encoder_final_state = tf.contrib.seq2seq.tile_batch(self.encoder_state, multiplier=self.beam_width)\n",
        "                tiled_seq_len = tf.contrib.seq2seq.tile_batch(self.X_len, multiplier=self.beam_width)\n",
        "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
        "                    self.num_hidden * 2, tiled_encoder_output, memory_sequence_length=tiled_seq_len, normalize=True)\n",
        "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,\n",
        "                                                                   attention_layer_size=self.num_hidden * 2)\n",
        "                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size * self.beam_width)\n",
        "                initial_state = initial_state.clone(cell_state=tiled_encoder_final_state)\n",
        "                decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
        "                    cell=decoder_cell,\n",
        "                    embedding=self.embeddings,\n",
        "                    start_tokens=tf.fill([self.batch_size], tf.constant(2)),\n",
        "                    end_token=tf.constant(3),\n",
        "                    initial_state=initial_state,\n",
        "                    beam_width=self.beam_width,\n",
        "                    output_layer=self.projection_layer\n",
        "                )\n",
        "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "                    decoder, output_time_major=True, maximum_iterations=summary_max_len, scope=decoder_scope)\n",
        "                self.prediction = tf.transpose(outputs.predicted_ids, perm=[1, 2, 0])\n",
        "        #print(\"Decoder output \",self.decoder_output.get_shape().as_list())\n",
        "        #print(\"Decoder input \",self.decoder_input.get_shape().as_list())\n",
        "        #print(\"input \",self.X.get_shape().as_list())\n",
        "\n",
        "        with tf.name_scope(\"loss\"):\n",
        "            if not forward_only:\n",
        "                crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                    logits=self.logits_reshape, labels=self.decoder_target)\n",
        "                weights = tf.sequence_mask(self.decoder_len, summary_max_len, dtype=tf.float32)\n",
        "                self.loss = tf.reduce_sum(crossent * weights / tf.to_float(self.batch_size))\n",
        "\n",
        "                params = tf.trainable_variables()\n",
        "                gradients = tf.gradients(self.loss, params)\n",
        "                clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
        "                optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
        "                self.update = optimizer.apply_gradients(zip(clipped_gradients, params), global_step=self.global_step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5GBwv5wZv6r",
        "colab_type": "code",
        "outputId": "e633b298-08ee-4777-a6da-b3c5214c201e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''import time\n",
        "start = time.perf_counter()\n",
        "import tensorflow as tf\n",
        "import argparse\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "class args:\n",
        "    pass\n",
        "  \n",
        "args.num_hidden=150\n",
        "args.num_layers=2\n",
        "args.beam_width=10\n",
        "args.glove=\"store_true\"\n",
        "args.embedding_size=300\n",
        "\n",
        "args.learning_rate=1e-3\n",
        "args.batch_size = 50\n",
        "args.num_epochs = 5\n",
        "args.keep_prob = 0.8\n",
        "\n",
        "args.toy = False #\"store_true\"\n",
        "\n",
        "args.with_model=\"store_true\"\n",
        "if not os.path.exists(default_path + \"saved_model\"):\n",
        "    os.mkdir(default_path + \"saved_model\")\n",
        "else:\n",
        "  old_model_checkpoint_path = open(default_path + 'saved_model/checkpoint', 'r')\n",
        "  old_model_checkpoint_path = \"\".join([default_path + \"saved_model/\",old_model_checkpoint_path.read().splitlines()[0].split('\"')[1] ])\n",
        "\n",
        "\n",
        "print(\"Building dictionary...\")\n",
        "word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"train\", args.toy)\n",
        "print(\"Loading training dataset...\")\n",
        "#train_x, train_y = build_dataset(\"train\", word_dict, article_max_len, summary_max_len, args.toy)\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    model = Model(reversed_dict, article_max_len, summary_max_len, args)\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    saver = tf.train.Saver(tf.global_variables())\n",
        "    if 'old_model_checkpoint_path' in globals():\n",
        "        print(\"Continuing from previous trained model:\" , old_model_checkpoint_path , \"...\")\n",
        "        saver.restore(sess, old_model_checkpoint_path )\n",
        "\n",
        "    #batches = batch_iter(train_x, train_y, args.batch_size, args.num_epochs)\n",
        "    num_batches_per_epoch = (input_size_) // args.batch_size\n",
        "\n",
        "    print(\"\\nIteration starts.\")\n",
        "    print(\"Number of batches per epoch :\", num_batches_per_epoch)\n",
        "    for inde in range(args.num_epochs):\n",
        "        with open(default_path + \"sumdata/train/train.article.txt\", \"r\", encoding=\"utf-8\") as f1, open(default_path + \"sumdata/train/train.title.txt\", \"r\", encoding=\"utf-8\") as f2:\n",
        "            print(\"Hello\")\n",
        "            inputs_ = list()\n",
        "            outputs_ = list()\n",
        "            for index, (inputs, outputs) in enumerate(zip(f1, f2)):\n",
        "                if((index+1) % args.batch_size == 0 ):\n",
        "                    #print(inputs_)\n",
        "                    inputss, outputss = build_dataset(\"train\", inputs_, outputs_, word_dict, article_max_len, summary_max_len, args.toy)\n",
        "                    #print(\"here it is working better\")\n",
        "                    inputss = np.array(inputss)\n",
        "                    outputss = np.array(outputss)\n",
        "                    #print(len(inputss))\n",
        "                    #print(len(outputss))\n",
        "                    #inputss = inputss[:,:]\n",
        "                    #outputs = outputs[:,:]\n",
        "                    #print(\"After \",len(inputss))\n",
        "                    #print(\"After \",len(outputss))\n",
        "                    batch_x_len = list(map(lambda x: len([y for y in x if y != 0]), inputss))\n",
        "                    batch_decoder_input = list(map(lambda x: [word_dict[\"<s>\"]] + list(x), outputss))\n",
        "                    batch_decoder_len = list(map(lambda x: len([y for y in x if y != 0]), batch_decoder_input))\n",
        "                    batch_decoder_output = list(map(lambda x: list(x) + [word_dict[\"</s>\"]], outputss))\n",
        "\n",
        "                    batch_decoder_input = list(\n",
        "                        map(lambda d: d + (summary_max_len - len(d)) * [word_dict[\"<padding>\"]], batch_decoder_input))\n",
        "                    batch_decoder_output = list(\n",
        "                        map(lambda d: d + (summary_max_len - len(d)) * [word_dict[\"<padding>\"]], batch_decoder_output))\n",
        "\n",
        "                    train_feed_dict = {\n",
        "                        model.batch_size: len(inputss),\n",
        "                        model.X: inputss,\n",
        "                        model.X_len: batch_x_len,\n",
        "                        model.decoder_input: batch_decoder_input,\n",
        "                        model.decoder_len: batch_decoder_len,\n",
        "                        model.decoder_target: batch_decoder_output\n",
        "                    }\n",
        "\n",
        "                    _, step, loss = sess.run([model.update, model.global_step, model.loss], feed_dict=train_feed_dict)\n",
        "                    #print(\"Hey it is running Prabha\")\n",
        "                    if step % 1000 == 0:\n",
        "                        print(\"step {0}: loss = {1}\".format(step, loss))\n",
        "                    print(\"step {0}: loss = {1}\".format(step, loss))\n",
        "                    if step % num_batches_per_epoch == 0:\n",
        "                        hours, rem = divmod(time.perf_counter() - start, 3600)\n",
        "                        minutes, seconds = divmod(rem, 60)\n",
        "                        saver.save(sess, default_path + \"saved_model/model.ckpt\", global_step=step)\n",
        "                        print(\" Epoch {0}: Model is saved.\".format(step // num_batches_per_epoch),\n",
        "                        \"Elapsed: {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds) , \"\\n\")\n",
        "                    inputs_, outputs_ = [],[]\n",
        "                    inputs_.append(inputs)\n",
        "                    outputs_.append(outputs)\n",
        "                else:\n",
        "                    inputs_.append(inputs)\n",
        "                    outputs_.append(outputs)'''"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import time\\nstart = time.perf_counter()\\nimport tensorflow as tf\\nimport argparse\\nimport pickle\\nimport os\\n\\nclass args:\\n    pass\\n  \\nargs.num_hidden=150\\nargs.num_layers=2\\nargs.beam_width=10\\nargs.glove=\"store_true\"\\nargs.embedding_size=300\\n\\nargs.learning_rate=1e-3\\nargs.batch_size = 50\\nargs.num_epochs = 5\\nargs.keep_prob = 0.8\\n\\nargs.toy = False #\"store_true\"\\n\\nargs.with_model=\"store_true\"\\nif not os.path.exists(default_path + \"saved_model\"):\\n    os.mkdir(default_path + \"saved_model\")\\nelse:\\n  old_model_checkpoint_path = open(default_path + \\'saved_model/checkpoint\\', \\'r\\')\\n  old_model_checkpoint_path = \"\".join([default_path + \"saved_model/\",old_model_checkpoint_path.read().splitlines()[0].split(\\'\"\\')[1] ])\\n\\n\\nprint(\"Building dictionary...\")\\nword_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"train\", args.toy)\\nprint(\"Loading training dataset...\")\\n#train_x, train_y = build_dataset(\"train\", word_dict, article_max_len, summary_max_len, args.toy)\\n\\ntf.reset_default_graph()\\n\\nwith tf.Session() as sess:\\n    model = Model(reversed_dict, article_max_len, summary_max_len, args)\\n    sess.run(tf.global_variables_initializer())\\n    saver = tf.train.Saver(tf.global_variables())\\n    if \\'old_model_checkpoint_path\\' in globals():\\n        print(\"Continuing from previous trained model:\" , old_model_checkpoint_path , \"...\")\\n        saver.restore(sess, old_model_checkpoint_path )\\n\\n    #batches = batch_iter(train_x, train_y, args.batch_size, args.num_epochs)\\n    num_batches_per_epoch = (input_size_) // args.batch_size\\n\\n    print(\"\\nIteration starts.\")\\n    print(\"Number of batches per epoch :\", num_batches_per_epoch)\\n    for inde in range(args.num_epochs):\\n        with open(default_path + \"sumdata/train/train.article.txt\", \"r\", encoding=\"utf-8\") as f1, open(default_path + \"sumdata/train/train.title.txt\", \"r\", encoding=\"utf-8\") as f2:\\n            print(\"Hello\")\\n            inputs_ = list()\\n            outputs_ = list()\\n            for index, (inputs, outputs) in enumerate(zip(f1, f2)):\\n                if((index+1) % args.batch_size == 0 ):\\n                    #print(inputs_)\\n                    inputss, outputss = build_dataset(\"train\", inputs_, outputs_, word_dict, article_max_len, summary_max_len, args.toy)\\n                    #print(\"here it is working better\")\\n                    inputss = np.array(inputss)\\n                    outputss = np.array(outputss)\\n                    #print(len(inputss))\\n                    #print(len(outputss))\\n                    #inputss = inputss[:,:]\\n                    #outputs = outputs[:,:]\\n                    #print(\"After \",len(inputss))\\n                    #print(\"After \",len(outputss))\\n                    batch_x_len = list(map(lambda x: len([y for y in x if y != 0]), inputss))\\n                    batch_decoder_input = list(map(lambda x: [word_dict[\"<s>\"]] + list(x), outputss))\\n                    batch_decoder_len = list(map(lambda x: len([y for y in x if y != 0]), batch_decoder_input))\\n                    batch_decoder_output = list(map(lambda x: list(x) + [word_dict[\"</s>\"]], outputss))\\n\\n                    batch_decoder_input = list(\\n                        map(lambda d: d + (summary_max_len - len(d)) * [word_dict[\"<padding>\"]], batch_decoder_input))\\n                    batch_decoder_output = list(\\n                        map(lambda d: d + (summary_max_len - len(d)) * [word_dict[\"<padding>\"]], batch_decoder_output))\\n\\n                    train_feed_dict = {\\n                        model.batch_size: len(inputss),\\n                        model.X: inputss,\\n                        model.X_len: batch_x_len,\\n                        model.decoder_input: batch_decoder_input,\\n                        model.decoder_len: batch_decoder_len,\\n                        model.decoder_target: batch_decoder_output\\n                    }\\n\\n                    _, step, loss = sess.run([model.update, model.global_step, model.loss], feed_dict=train_feed_dict)\\n                    #print(\"Hey it is running Prabha\")\\n                    if step % 1000 == 0:\\n                        print(\"step {0}: loss = {1}\".format(step, loss))\\n                    print(\"step {0}: loss = {1}\".format(step, loss))\\n                    if step % num_batches_per_epoch == 0:\\n                        hours, rem = divmod(time.perf_counter() - start, 3600)\\n                        minutes, seconds = divmod(rem, 60)\\n                        saver.save(sess, default_path + \"saved_model/model.ckpt\", global_step=step)\\n                        print(\" Epoch {0}: Model is saved.\".format(step // num_batches_per_epoch),\\n                        \"Elapsed: {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds) , \"\\n\")\\n                    inputs_, outputs_ = [],[]\\n                    inputs_.append(inputs)\\n                    outputs_.append(outputs)\\n                else:\\n                    inputs_.append(inputs)\\n                    outputs_.append(outputs)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBtObYX13Yn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_text_list_test(data_path, toy,input_size_):\n",
        "    with open (data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        if not toy:\n",
        "            return [clean_str(x.strip()) for x in f.readlines()][:input_size_]\n",
        "        else:\n",
        "            return [clean_str(x.strip()) for x in f.readlines()][:1000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPOJap43bAav",
        "colab_type": "code",
        "outputId": "a8b2014e-2c75-47c4-d434-98500fe6d147",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import pickle\n",
        "#from model import Model\n",
        "#from utils import build_dict, build_dataset, batch_iter\n",
        "\n",
        "#path for validation text (article)\n",
        "valid_article_path = default_path + \"sumdata/train/valid.article.filter.txt\"\n",
        "\n",
        "#path for validation text output(headline)\n",
        "valid_title_path   = default_path + \"sumdata/train/valid.title.filter.txt\"\n",
        "\n",
        "\n",
        "#with open(\"args.pickle\", \"rb\") as f:\n",
        "#    args = pickle.load(f)\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "class args:\n",
        "    pass\n",
        "  \n",
        "args.num_hidden=150\n",
        "args.num_layers=2\n",
        "args.beam_width=10\n",
        "args.glove=\"store_true\"\n",
        "args.embedding_size = 300\n",
        "\n",
        "args.learning_rate=1e-3\n",
        "args.batch_size= 1\n",
        "args.num_epochs= 1\n",
        "args.keep_prob = 0.8\n",
        "\n",
        "args.toy = False\n",
        "\n",
        "args.with_model=\"store_true\"\n",
        "args.input_size_ = 1\n",
        "\n",
        "\n",
        "print(\"Loading dictionary...\")\n",
        "print(\"Loading dictionary...\")\n",
        "word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"valid\", args.toy)\n",
        "print(\"Loading validation dataset...\")\n",
        "#valid_x = build_dataset(\"valid\", word_dict, article_max_len, summary_max_len, args.toy)\n",
        "#valid_x_len = [len([y for y in x if y != 0]) for x in valid_x]\n",
        "print(\"Loading article and reference...\")\n",
        "article = get_text_list_test(valid_article_path, args.toy, args.input_size_)\n",
        "reference = get_text_list_test(valid_title_path, args.toy, args.input_size_)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    print(\"Loading saved model...\")\n",
        "    model = Model(reversed_dict, article_max_len, summary_max_len, args, forward_only=True)\n",
        "    saver = tf.train.Saver(tf.global_variables())\n",
        "    ckpt = tf.train.get_checkpoint_state(default_path + \"saved_model/\")\n",
        "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "\n",
        "    #batches = batch_iter(valid_x, [0] * len(valid_x), args.batch_size, 1)\n",
        "\n",
        "    print(\"Writing summaries to 'result.txt'...\")\n",
        "    for inde in range(args.num_epochs):\n",
        "        with open(default_path + \"sumdata/train/valid.article.filter.txt\", \"r\", encoding=\"utf-8\") as f1:\n",
        "            print(\"Hello\")\n",
        "            inputs_ = list()\n",
        "            outputs_ = list()\n",
        "            for index, inputs in enumerate(f1):\n",
        "                if((index+1) % args.batch_size == 0 ):\n",
        "                    #print(inputs_)\n",
        "                    inputs_.append(inputs)\n",
        "                    #outputs_.append(outputs)\n",
        "                    inputss = build_dataset(\"valid\", inputs_, outputs_, word_dict, article_max_len, summary_max_len)\n",
        "                    print(\"here it is working better\")\n",
        "                    inputss = np.array(inputss)\n",
        "                    batch_x_len = list(map(lambda x: len([y for y in x if y != 0]), inputss))\n",
        "                    valid_feed_dict = {\n",
        "                        model.batch_size: len(inputss),\n",
        "                        model.X: inputss,\n",
        "                        model.X_len: batch_x_len,\n",
        "                    }\n",
        "                    prediction = sess.run(model.prediction, feed_dict=valid_feed_dict)\n",
        "                    prediction_output = [[reversed_dict[y] for y in x] for x in prediction[:, 0, :]]\n",
        "                    summary_array = []\n",
        "                    with open(default_path + \"result.txt\", \"w+\") as f:\n",
        "                        for line in prediction_output:\n",
        "                            summary = list()\n",
        "                            summari = ''\n",
        "                            for word in line:\n",
        "                                if word == \"</s>\":\n",
        "                                    break\n",
        "                                if word not in summary:\n",
        "                                    summary.append(word)\n",
        "                                    summari = summari + word + ' '\n",
        "                            summary_array.append(\" \".join(summary))\n",
        "                            #print(\" \".join(summary), file=f)\n",
        "                            f.write(summari + \"\\n\")\n",
        "                    inputs_, outputs_ = [],[]\n",
        "                else:\n",
        "                    inputs_.append(inputs)\n",
        "                    #outputs_.append(outputs)\n",
        "                if index == 2:\n",
        "                    break\n",
        "\n",
        "    print('Summaries have been generated')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading dictionary...\n",
            "Loading dictionary...\n",
            "Loading validation dataset...\n",
            "Loading article and reference...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0710 06:32:16.213886 140553886504832 deprecation.py:323] From <ipython-input-11-d18ee4500905>:35: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "W0710 06:32:16.231102 140553886504832 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/rnn/python/ops/rnn.py:239: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "W0710 06:32:16.233667 140553886504832 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "W0710 06:32:16.340153 140553886504832 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0710 06:32:16.356303 140553886504832 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading saved model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0710 06:32:16.786676 140553886504832 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0710 06:32:19.040442 140553886504832 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py:985: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0710 06:32:19.832315 140553886504832 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Writing summaries to 'result.txt'...\n",
            "Hello\n",
            "here it is working better\n",
            "here it is working better\n",
            "here it is working better\n",
            "Summaries have been generated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fpuryfr76o5q",
        "colab_type": "code",
        "outputId": "152b9351-c532-4971-a3a3-b27fe45fdc01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(len(summary_array))\n",
        "print(summary_array[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "gm sales down # . percent in december\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYcY2k4gbXww",
        "colab_type": "code",
        "outputId": "6e615a32-475e-448e-924e-41f350e291b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "!pip3 install sumeval\n",
        "!python3 -m spacy download en"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sumeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/d0/23c9a37253044d478efede0770aefaea52a5b75911da56c3ac0aca894d2a/sumeval-0.2.0.tar.gz (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: plac>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from sumeval) (0.9.6)\n",
            "Collecting sacrebleu>=1.3.2 (from sumeval)\n",
            "  Downloading https://files.pythonhosted.org/packages/c3/5a/a1949ecc69ab17b6d1ef06d01ff0e5ada0a2bad8ffcdd3f597ad375e71dd/sacrebleu-1.3.6.tar.gz\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.3.2->sumeval) (3.7.4)\n",
            "Building wheels for collected packages: sumeval, sacrebleu\n",
            "  Building wheel for sumeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/9b/50/fda7087af57d2efde31d4a85cecda98e683ed58eee07fbff2b\n",
            "  Building wheel for sacrebleu (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/58/be/1915daa86d8a7c9c1134754846c80f481b8d352329f7bd7fa1\n",
            "Successfully built sumeval sacrebleu\n",
            "Installing collected packages: sacrebleu, sumeval\n",
            "Successfully installed sacrebleu-1.3.6 sumeval-0.2.0\n",
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rkkBxdyjvUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sumeval.metrics.rouge import RougeCalculator\n",
        "from sumeval.metrics.bleu import BLEUCalculator\n",
        "\n",
        "def eval_rouges(refrence_summary,model_summary):\n",
        "    #refrence_summary = \"tokyo shares close up #.## percent\"\n",
        "    #model_summary = \"tokyo stocks close up # percent to fresh record high\"\n",
        "\n",
        "    rouge = RougeCalculator(stopwords=True, lang=\"en\")\n",
        "\n",
        "    rouge_1 = rouge.rouge_n(\n",
        "                summary=model_summary,\n",
        "                references=refrence_summary,\n",
        "                n=1)\n",
        "\n",
        "    rouge_2 = rouge.rouge_n(\n",
        "                summary=model_summary,\n",
        "                references=[refrence_summary],\n",
        "                n=2)\n",
        "    \n",
        "    rouge_l = rouge.rouge_l(\n",
        "                summary=model_summary,\n",
        "                references=[refrence_summary])\n",
        "    \n",
        "    # You need spaCy to calculate ROUGE-BE\n",
        "    \n",
        "    rouge_be = rouge.rouge_be(\n",
        "                summary=model_summary,\n",
        "                references=[refrence_summary])\n",
        "\n",
        "    bleu = BLEUCalculator()\n",
        "    bleu_score = bleu.bleu( summary=model_summary,\n",
        "                        references=[refrence_summary])\n",
        "    \n",
        "    return rouge_1, rouge_2,rouge_l,rouge_be,bleu_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fClzTNaTj8NM",
        "colab_type": "code",
        "outputId": "347764d2-68c2-4476-cd0b-7ebbc1a658bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "bleu_arr = []\n",
        "rouge_1_arr  = []\n",
        "rouge_2_arr  = []\n",
        "rouge_L_arr  = []\n",
        "rouge_be_arr = []\n",
        "\n",
        "from xml.etree import ElementTree\n",
        "from xml.dom import minidom\n",
        "from functools import reduce\n",
        "\n",
        "def prettify(elem):\n",
        "    \"\"\"Return a pretty-printed XML string for the Element.\n",
        "    \"\"\"\n",
        "    rough_string = ElementTree.tostring(elem, 'utf-8')\n",
        "    reparsed = minidom.parseString(rough_string)\n",
        "    return reparsed.toprettyxml(indent=\"  \")\n",
        "  \n",
        "from xml.etree.ElementTree import Element, SubElement, Comment\n",
        "\n",
        "top = Element('ZakSum')\n",
        "\n",
        "comment = Comment('Generated by Prbha Janagama')\n",
        "top.append(comment)\n",
        "\n",
        "i=0\n",
        "for summ in summary_array:\n",
        "  example = SubElement(top, 'example')\n",
        "  article_element   = SubElement(example, 'article')\n",
        "  article_element.text = article[i]\n",
        "  \n",
        "  reference_element = SubElement(example, 'reference')\n",
        "  reference_element.text = reference[i]\n",
        "  \n",
        "  summary_element   = SubElement(example, 'summary')\n",
        "  summary_element.text = summ\n",
        "\n",
        "  rouge_1, rouge_2,rouge_L,rouge_be,bleu_score = eval_rouges(reference[i],summ )\n",
        "  print(\"Rouge 1 : \",rouge_1)\n",
        "  print(\"Rouge 2 : \",rouge_2)\n",
        "  print(\"Rouge L : \",rouge_L)\n",
        "  \n",
        "  eval_element = SubElement(example, 'eval')\n",
        "  bleu_score_element = SubElement(eval_element,'BLEU', {'score':str(bleu_score)})\n",
        "  ROUGE_1_element  = SubElement(eval_element, 'ROUGE_1' , {'score':str(rouge_1)})\n",
        "  ROUGE_2_element  = SubElement(eval_element, 'ROUGE_2' , {'score':str(rouge_2)})\n",
        "  ROUGE_L_element  = SubElement(eval_element, 'ROUGE_l' , {'score':str(rouge_L)})\n",
        "  ROUGE_be_element  = SubElement(eval_element,'ROUGE_be', {'score':str(rouge_be)})\n",
        "  \n",
        "  bleu_arr.append(bleu_score) \n",
        "  rouge_1_arr.append(rouge_1) \n",
        "  rouge_2_arr.append(rouge_2) \n",
        "  rouge_L_arr.append(rouge_L) \n",
        "  rouge_be_arr.append(rouge_be) \n",
        "\n",
        "  i+=1\n",
        "  if(i==1):\n",
        "    break\n",
        "\n",
        "top.set('bleu', str(reduce(lambda x, y: x + y,  bleu_arr) / len(bleu_arr)))\n",
        "top.set('rouge_1', str(reduce(lambda x, y: x + y,  rouge_1_arr) / len(rouge_1_arr)))\n",
        "top.set('rouge_2', str(reduce(lambda x, y: x + y,  rouge_2_arr) / len(rouge_2_arr)))\n",
        "top.set('rouge_L', str(reduce(lambda x, y: x + y,  rouge_L_arr) / len(rouge_L_arr)))\n",
        "top.set('rouge_be', str(reduce(lambda x, y: x + y, rouge_be_arr) / len(rouge_be_arr)))\n",
        "\n",
        "with open(default_path + \"result_valid_29_10_2018_5_28pm.xml\", \"w+\") as f:\n",
        "  print(prettify(top), file=f)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a.sales=(pobj)=>gm\n",
            "<BasicElement: sales-[pobj]->gm>\n",
            "a.percent=(npadvmod)=>gm\n",
            "<BasicElement: percent-[npadvmod]->gm>\n",
            "a.injury=(nsubj)=>leaves\n",
            "<BasicElement: injury-[nsubj]->leave>\n",
            "a.kwan=(dobj)=>leaves\n",
            "<BasicElement: kwan-[dobj]->leave>\n",
            "b.olympic=(amod)=>hopes\n",
            "Rouge 1 :  0\n",
            "Rouge 2 :  0\n",
            "Rouge L :  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJ-N1s5ABLp0",
        "colab_type": "code",
        "outputId": "cdf83b48-bba7-491f-f561-e43cb7e10460",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print(summary_array[0])\n",
        "print(reference[0])\n",
        "print(article[0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gm sales down # . percent in december\n",
            "injury leaves kwan 's olympic hopes in limbo\n",
            "five-time world champion michelle kwan withdrew from the # us figure skating championships on wednesday , but will petition us skating officials for the chance to compete at the # turin olympics .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zhpyv4dBRKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}